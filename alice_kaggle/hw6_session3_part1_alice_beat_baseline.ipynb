{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Open Machine Learning Course\n",
    "<center>\n",
    "Author: Yury Kashnitsky, Data Scientist at Mail.Ru Group\n",
    "\n",
    "This material is subject to the terms and conditions of the license [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Free use is permitted for any non-comercial purpose with an obligatory indication of the names of the authors and of the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Assignment #6. Part 1\n",
    "### <center> Beating benchmarks in \"Catch Me If You Can: Intruder Detection through Webpage Session Tracking\"\n",
    "    \n",
    "[Competition](https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2). The task is to beat \"Assignment 6 baseline\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '/home/andrei/Desktop/alice_kaggle'\n",
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n",
    "#train_df = train_df.loc[:10000]\n",
    "#test_df = test_df.loc[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate target feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>site6</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>2014-02-20 10:02:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>942.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>3846.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>3846.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>2014-02-22 11:20:15</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>2014-02-22 11:20:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>2013-12-16 16:40:17</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013-12-16 16:40:18</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14769.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:20</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:21</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:22</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:52:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:52:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:54:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:54:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>2014-02-28 10:53:05</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:57:06</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:57:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2    site3  \\\n",
       "session_id                                                                  \n",
       "1             718 2014-02-20 10:02:45    NaN                 NaT      NaN   \n",
       "2             890 2014-02-22 11:19:50  941.0 2014-02-22 11:19:50   3847.0   \n",
       "3           14769 2013-12-16 16:40:17   39.0 2013-12-16 16:40:18  14768.0   \n",
       "4             782 2014-03-28 10:52:12  782.0 2014-03-28 10:52:42    782.0   \n",
       "5              22 2014-02-28 10:53:05  177.0 2014-02-28 10:55:22    175.0   \n",
       "\n",
       "                         time3    site4               time4  site5  \\\n",
       "session_id                                                           \n",
       "1                          NaT      NaN                 NaT    NaN   \n",
       "2          2014-02-22 11:19:51    941.0 2014-02-22 11:19:51  942.0   \n",
       "3          2013-12-16 16:40:19  14769.0 2013-12-16 16:40:19   37.0   \n",
       "4          2014-03-28 10:53:12    782.0 2014-03-28 10:53:42  782.0   \n",
       "5          2014-02-28 10:55:22    178.0 2014-02-28 10:55:23  177.0   \n",
       "\n",
       "                         time5   site6               time6    site7  \\\n",
       "session_id                                                            \n",
       "1                          NaT     NaN                 NaT      NaN   \n",
       "2          2014-02-22 11:19:51  3846.0 2014-02-22 11:19:51   3847.0   \n",
       "3          2013-12-16 16:40:19    39.0 2013-12-16 16:40:19  14768.0   \n",
       "4          2014-03-28 10:54:12   782.0 2014-03-28 10:54:42    782.0   \n",
       "5          2014-02-28 10:55:23   178.0 2014-02-28 10:55:59    175.0   \n",
       "\n",
       "                         time7    site8               time8    site9  \\\n",
       "session_id                                                             \n",
       "1                          NaT      NaN                 NaT      NaN   \n",
       "2          2014-02-22 11:19:52   3846.0 2014-02-22 11:19:52   1516.0   \n",
       "3          2013-12-16 16:40:20  14768.0 2013-12-16 16:40:21  14768.0   \n",
       "4          2014-03-28 10:55:12    782.0 2014-03-28 10:55:42    782.0   \n",
       "5          2014-02-28 10:55:59    177.0 2014-02-28 10:55:59    177.0   \n",
       "\n",
       "                         time9   site10              time10  \n",
       "session_id                                                   \n",
       "1                          NaT      NaN                 NaT  \n",
       "2          2014-02-22 11:20:15   1518.0 2014-02-22 11:20:16  \n",
       "3          2013-12-16 16:40:22  14768.0 2013-12-16 16:40:24  \n",
       "4          2014-03-28 10:56:12    782.0 2014-03-28 10:56:42  \n",
       "5          2014-02-28 10:57:06    178.0 2014-02-28 10:57:11  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = train_df['target']\n",
    "train_df.drop('target', axis=1, inplace=True)\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "train_df[times] = train_df[times].apply(pd.to_datetime)\n",
    "test_df[times] = test_df[times].apply(pd.to_datetime)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всего сайтов: 48371\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>img.profilsearch.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39985</th>\n",
       "      <td>www.decouverte.canalsat.fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11485</th>\n",
       "      <td>ravel2008.mediactive.fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15324</th>\n",
       "      <td>cd32.static.jango.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3648</th>\n",
       "      <td>api.solvemedia.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             site\n",
       "5449         img.profilsearch.com\n",
       "39985  www.decouverte.canalsat.fr\n",
       "11485     ravel2008.mediactive.fr\n",
       "15324       cd32.static.jango.com\n",
       "3648           api.solvemedia.com"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загрузим словарик сайтов\n",
    "with open(r\"/home/andrei/Desktop/alice_kaggle/site_dic.pkl\", \"rb\") as input_file:\n",
    "    site_dict = pickle.load(input_file)\n",
    "\n",
    "# датафрейм словарика сайтов\n",
    "sites_dict_df = pd.DataFrame(list(site_dict.keys()), \n",
    "                          index=list(site_dict.values()), \n",
    "                          columns=['site'])\n",
    "print(u'всего сайтов:', sites_dict_df.shape[0])\n",
    "sites_dict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            site1               time1  site2               time2    site3  \\\n",
      "session_id                                                                  \n",
      "1             718 2014-02-20 10:02:45    NaN                 NaT      NaN   \n",
      "2             890 2014-02-22 11:19:50  941.0 2014-02-22 11:19:50   3847.0   \n",
      "3           14769 2013-12-16 16:40:17   39.0 2013-12-16 16:40:18  14768.0   \n",
      "4             782 2014-03-28 10:52:12  782.0 2014-03-28 10:52:42    782.0   \n",
      "5              22 2014-02-28 10:53:05  177.0 2014-02-28 10:55:22    175.0   \n",
      "\n",
      "                         time3    site4               time4  site5  \\\n",
      "session_id                                                           \n",
      "1                          NaT      NaN                 NaT    NaN   \n",
      "2          2014-02-22 11:19:51    941.0 2014-02-22 11:19:51  942.0   \n",
      "3          2013-12-16 16:40:19  14769.0 2013-12-16 16:40:19   37.0   \n",
      "4          2014-03-28 10:53:12    782.0 2014-03-28 10:53:42  782.0   \n",
      "5          2014-02-28 10:55:22    178.0 2014-02-28 10:55:23  177.0   \n",
      "\n",
      "                         time5         ...                      site_column1  \\\n",
      "session_id                             ...                                     \n",
      "1                          NaT         ...           rr.office.microsoft.com   \n",
      "2          2014-02-22 11:19:51         ...                   maps.google.com   \n",
      "3          2013-12-16 16:40:19         ...               cbk1.googleapis.com   \n",
      "4          2014-03-28 10:54:12         ...                    annotathon.org   \n",
      "5          2014-02-28 10:55:23         ...                   apis.google.com   \n",
      "\n",
      "                   site_column2         site_column3         site_column4  \\\n",
      "session_id                                                                  \n",
      "1                           NaN                  NaN                  NaN   \n",
      "2               mts0.google.com     khms0.google.com      mts0.google.com   \n",
      "3           accounts.google.com  cbk0.googleapis.com  cbk1.googleapis.com   \n",
      "4                annotathon.org       annotathon.org       annotathon.org   \n",
      "5              fr.wikipedia.org   bits.wikimedia.org   meta.wikimedia.org   \n",
      "\n",
      "                site_column5         site_column6         site_column7  \\\n",
      "session_id                                                               \n",
      "1                        NaN                  NaN                  NaN   \n",
      "2            mts1.google.com     khms1.google.com     khms0.google.com   \n",
      "3                twitter.com  accounts.google.com  cbk0.googleapis.com   \n",
      "4             annotathon.org       annotathon.org       annotathon.org   \n",
      "5           fr.wikipedia.org   meta.wikimedia.org   bits.wikimedia.org   \n",
      "\n",
      "                   site_column8         site_column9        site_column10  \n",
      "session_id                                                                 \n",
      "1                           NaN                  NaN                  NaN  \n",
      "2              khms1.google.com       193.164.197.30       193.164.196.60  \n",
      "3           cbk0.googleapis.com  cbk0.googleapis.com  cbk0.googleapis.com  \n",
      "4                annotathon.org       annotathon.org       annotathon.org  \n",
      "5              fr.wikipedia.org     fr.wikipedia.org   meta.wikimedia.org  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=100000, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_columns = ['site_column%s' % i for i in range(1, 11)]\n",
    "train_df['text_col'] = ''\n",
    "test_df['text_col'] = ''\n",
    "\n",
    "for i in range(1, 11):\n",
    "    site_c = 'site{}'.format(i)\n",
    "    site_name_c = 'site_column{}'.format(i)\n",
    "    train_df[site_name_c] = sites_dict_df.loc[train_df[site_c]].values\n",
    "    test_df[site_name_c] = sites_dict_df.loc[test_df[site_c]].values\n",
    "    train_df['text_col'] += train_df[site_name_c]\n",
    "    test_df['text_col'] += test_df[site_name_c]\n",
    "    \n",
    "print(train_df.head())\n",
    "\n",
    "all_train_text = pd.concat([train_df['site_column{}'.format(i)].astype('U') for i in range(1, 11)])  \n",
    "char_vec = TfidfVectorizer(analyzer='char', ngram_range=(1, 3), max_features=100000)\n",
    "word_vec = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), max_features=100000)\n",
    "# fix wordofvec\n",
    "word_vec.fit(all_train_text.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253561, 100000)\n",
      "(82797, 100000)\n"
     ]
    }
   ],
   "source": [
    "train_tfidf = [word_vec.transform(train_df['text_col'].values.astype('U'))]\n",
    "train_X = hstack(train_tfidf, format='csr')\n",
    "print(train_X.shape)\n",
    "test_tfidf = [word_vec.transform(test_df['text_col'].values.astype('U'))]\n",
    "test_X = hstack(test_tfidf, format='csr')\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Tf-Idf features based on sites. You can use `ngram_range`=(1, 3) and `max_features`=100000 or more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add features based on the session start time: hour, whether it's morning, day or night and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale this features and combine then with Tf-Idf based on sites (you'll need `scipy.sparse.hstack`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_common = dict()\n",
    "for i in range(1, 11):\n",
    "    a = train_df[y == 0]['site_column%s' % i].value_counts()\n",
    "    for k, v in dict(a).items():\n",
    "        alice_common.setdefault(k, 0)\n",
    "        alice_common[k] += v\n",
    "alice_common = list(alice_common.items())\n",
    "alice_common.sort(key=lambda x: x[1], reverse=True)\n",
    "alice_common = set([_[0] for _ in alice_common[:30]])\n",
    "\n",
    "\n",
    "def get_part_from_hour(h):\n",
    "    if h < 6:\n",
    "        return 0\n",
    "    if h < 12:\n",
    "        return 1\n",
    "    if h < 18:\n",
    "        return 2\n",
    "    return 3\n",
    "\n",
    "\n",
    "def get_session_duration(row):\n",
    "    time_values = row[['time%s' % i for i in range(1, 10)]].dropna().values\n",
    "    duration = (time_values.max() - time_values.min()).total_seconds()\n",
    "    return duration\n",
    "\n",
    "\n",
    "def get_count_of_good_sites(row):\n",
    "    res = 0\n",
    "    for i in range(1, 11):\n",
    "        res += row['site_column%s' % i] in alice_common\n",
    "    return res\n",
    "\n",
    "\n",
    "def add_extra_features(df):\n",
    "    df['hour'] = df['time1'].dt.hour\n",
    "    \n",
    "    for i in range(2, 7):\n",
    "        df['delta%s' % (i-1)] = (df['time%s' % i] - df['time%s' % (i-1)]).dt.total_seconds()\n",
    "\n",
    "    df['duration'] = df.apply(get_session_duration, axis=1)\n",
    "    \n",
    "    df['count_good'] = df.apply(get_count_of_good_sites, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_scaled_features(df):\n",
    "    scalable_columns = ['hour', 'delta1', 'delta2', 'delta3',\n",
    "       'delta4', 'delta5', 'duration', 'count_good']\n",
    "    return StandardScaler().fit_transform(df[scalable_columns].fillna(-1))\n",
    "\n",
    "\n",
    "def get_part_of_day(df):\n",
    "    df['part_of_day'] = np.array(list(map(lambda v: get_part_from_hour(v), df['time1'].dt.hour)))\n",
    "    return OneHotEncoder().fit_transform(df[['part_of_day']])\n",
    "\n",
    "\n",
    "def get_extra_features(df):\n",
    "    return get_scaled_features(add_extra_features(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform cross-validation with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_part_of_day(train_df)\n",
    "train_X = hstack([train_X, get_extra_features(train_df), get_part_of_day(train_df)], format='csr')\n",
    "test_X = hstack([test_X, get_extra_features(test_df), get_part_of_day(test_df)], format='csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction for the test set and form a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 8 candidates, totalling 56 fits\n",
      "[CV] C=0.0001 ........................................................\n",
      "[CV] ......................................... C=0.0001, total=   0.7s\n",
      "[CV] C=0.0001 ........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................................... C=0.0001, total=   0.7s\n",
      "[CV] C=0.0001 ........................................................\n",
      "[CV] ......................................... C=0.0001, total=   0.7s\n",
      "[CV] C=0.0001 ........................................................\n",
      "[CV] ......................................... C=0.0001, total=   0.7s\n",
      "[CV] C=0.0001 ........................................................\n",
      "[CV] ......................................... C=0.0001, total=   0.7s\n",
      "[CV] C=0.0001 ........................................................\n",
      "[CV] ......................................... C=0.0001, total=   0.7s\n",
      "[CV] C=0.0001 ........................................................\n",
      "[CV] ......................................... C=0.0001, total=   0.7s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......................................... C=0.001, total=   0.9s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......................................... C=0.001, total=   0.9s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......................................... C=0.001, total=   0.9s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......................................... C=0.001, total=   0.9s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......................................... C=0.001, total=   0.9s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......................................... C=0.001, total=   0.9s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......................................... C=0.001, total=   0.9s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........................................... C=0.01, total=   1.5s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........................................... C=0.01, total=   1.5s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........................................... C=0.01, total=   1.5s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........................................... C=0.01, total=   1.5s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........................................... C=0.01, total=   1.5s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........................................... C=0.01, total=   1.5s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........................................... C=0.01, total=   1.5s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................................ C=0.1, total=   2.4s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................................ C=0.1, total=   2.4s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................................ C=0.1, total=   2.1s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................................ C=0.1, total=   2.3s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................................ C=0.1, total=   2.4s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................................ C=0.1, total=   2.2s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................................ C=0.1, total=   2.5s\n",
      "[CV] C=1.0 ...........................................................\n",
      "[CV] ............................................ C=1.0, total=   4.2s\n",
      "[CV] C=1.0 ...........................................................\n",
      "[CV] ............................................ C=1.0, total=   5.3s\n",
      "[CV] C=1.0 ...........................................................\n",
      "[CV] ............................................ C=1.0, total=   4.5s\n",
      "[CV] C=1.0 ...........................................................\n",
      "[CV] ............................................ C=1.0, total=   6.2s\n",
      "[CV] C=1.0 ...........................................................\n",
      "[CV] ............................................ C=1.0, total=   7.1s\n",
      "[CV] C=1.0 ...........................................................\n",
      "[CV] ............................................ C=1.0, total=   4.5s\n",
      "[CV] C=1.0 ...........................................................\n",
      "[CV] ............................................ C=1.0, total=   4.7s\n",
      "[CV] C=10.0 ..........................................................\n",
      "[CV] ........................................... C=10.0, total=   9.0s\n",
      "[CV] C=10.0 ..........................................................\n",
      "[CV] ........................................... C=10.0, total=  11.4s\n",
      "[CV] C=10.0 ..........................................................\n",
      "[CV] ........................................... C=10.0, total=   8.7s\n",
      "[CV] C=10.0 ..........................................................\n",
      "[CV] ........................................... C=10.0, total=  10.0s\n",
      "[CV] C=10.0 ..........................................................\n",
      "[CV] ........................................... C=10.0, total=  11.7s\n",
      "[CV] C=10.0 ..........................................................\n",
      "[CV] ........................................... C=10.0, total=   9.0s\n",
      "[CV] C=10.0 ..........................................................\n",
      "[CV] ........................................... C=10.0, total=  10.8s\n",
      "[CV] C=100.0 .........................................................\n",
      "[CV] .......................................... C=100.0, total=  16.2s\n",
      "[CV] C=100.0 .........................................................\n",
      "[CV] .......................................... C=100.0, total=  20.3s\n",
      "[CV] C=100.0 .........................................................\n",
      "[CV] .......................................... C=100.0, total=  17.3s\n",
      "[CV] C=100.0 .........................................................\n",
      "[CV] .......................................... C=100.0, total=  26.3s\n",
      "[CV] C=100.0 .........................................................\n",
      "[CV] .......................................... C=100.0, total=  20.8s\n",
      "[CV] C=100.0 .........................................................\n",
      "[CV] .......................................... C=100.0, total=  17.7s\n",
      "[CV] C=100.0 .........................................................\n",
      "[CV] .......................................... C=100.0, total=  19.0s\n",
      "[CV] C=1000.0 ........................................................\n",
      "[CV] ......................................... C=1000.0, total=  52.4s\n",
      "[CV] C=1000.0 ........................................................\n",
      "[CV] ......................................... C=1000.0, total=  44.9s\n",
      "[CV] C=1000.0 ........................................................\n",
      "[CV] ......................................... C=1000.0, total=  45.8s\n",
      "[CV] C=1000.0 ........................................................\n",
      "[CV] ......................................... C=1000.0, total=  50.3s\n",
      "[CV] C=1000.0 ........................................................\n",
      "[CV] ......................................... C=1000.0, total=  22.4s\n",
      "[CV] C=1000.0 ........................................................\n",
      "[CV] ......................................... C=1000.0, total=  42.0s\n",
      "[CV] C=1000.0 ........................................................\n",
      "[CV] ......................................... C=1000.0, total=  50.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  56 out of  56 | elapsed:  9.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=7, random_state=17, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=17, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(n_jobs=-1, random_state=17)\n",
    "\n",
    "log_params = {'C': list(np.power(10.0, np.arange(-4, 4)))}\n",
    "skf = StratifiedKFold(n_splits=7, shuffle=True, random_state=17)\n",
    "lgt_grid = GridSearchCV(logit, log_params, cv=skf, scoring='roc_auc', verbose=2)\n",
    "lgt_grid.fit(train_X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9698634955782192"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgt_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lgt_grid.predict_proba(test_X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(test_pred, \"assignment6_alice_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}