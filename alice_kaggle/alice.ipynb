{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>site6</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>2014-02-20 10:02:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>942.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>3846.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>3846.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>2014-02-22 11:20:15</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>2014-02-22 11:20:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>2013-12-16 16:40:17</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013-12-16 16:40:18</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14769.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:20</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:21</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:22</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:52:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:52:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:54:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:54:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>2014-02-28 10:53:05</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:57:06</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:57:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2    site3  \\\n",
       "session_id                                                                  \n",
       "1             718 2014-02-20 10:02:45    NaN                 NaT      NaN   \n",
       "2             890 2014-02-22 11:19:50  941.0 2014-02-22 11:19:50   3847.0   \n",
       "3           14769 2013-12-16 16:40:17   39.0 2013-12-16 16:40:18  14768.0   \n",
       "4             782 2014-03-28 10:52:12  782.0 2014-03-28 10:52:42    782.0   \n",
       "5              22 2014-02-28 10:53:05  177.0 2014-02-28 10:55:22    175.0   \n",
       "\n",
       "                         time3    site4               time4  site5  \\\n",
       "session_id                                                           \n",
       "1                          NaT      NaN                 NaT    NaN   \n",
       "2          2014-02-22 11:19:51    941.0 2014-02-22 11:19:51  942.0   \n",
       "3          2013-12-16 16:40:19  14769.0 2013-12-16 16:40:19   37.0   \n",
       "4          2014-03-28 10:53:12    782.0 2014-03-28 10:53:42  782.0   \n",
       "5          2014-02-28 10:55:22    178.0 2014-02-28 10:55:23  177.0   \n",
       "\n",
       "                         time5   site6               time6    site7  \\\n",
       "session_id                                                            \n",
       "1                          NaT     NaN                 NaT      NaN   \n",
       "2          2014-02-22 11:19:51  3846.0 2014-02-22 11:19:51   3847.0   \n",
       "3          2013-12-16 16:40:19    39.0 2013-12-16 16:40:19  14768.0   \n",
       "4          2014-03-28 10:54:12   782.0 2014-03-28 10:54:42    782.0   \n",
       "5          2014-02-28 10:55:23   178.0 2014-02-28 10:55:59    175.0   \n",
       "\n",
       "                         time7    site8               time8    site9  \\\n",
       "session_id                                                             \n",
       "1                          NaT      NaN                 NaT      NaN   \n",
       "2          2014-02-22 11:19:52   3846.0 2014-02-22 11:19:52   1516.0   \n",
       "3          2013-12-16 16:40:20  14768.0 2013-12-16 16:40:21  14768.0   \n",
       "4          2014-03-28 10:55:12    782.0 2014-03-28 10:55:42    782.0   \n",
       "5          2014-02-28 10:55:59    177.0 2014-02-28 10:55:59    177.0   \n",
       "\n",
       "                         time9   site10              time10  \n",
       "session_id                                                   \n",
       "1                          NaT      NaN                 NaT  \n",
       "2          2014-02-22 11:20:15   1518.0 2014-02-22 11:20:16  \n",
       "3          2013-12-16 16:40:22  14768.0 2013-12-16 16:40:24  \n",
       "4          2014-03-28 10:56:12    782.0 2014-03-28 10:56:42  \n",
       "5          2014-02-28 10:57:06    178.0 2014-02-28 10:57:11  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_DATA = '/home/andrei/Desktop/alice_kaggle'\n",
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n",
    "train_df.sort_values('time1')\n",
    "#train_df = train_df.loc[:10000]\n",
    "#test_df = test_df.loc[:10000]\n",
    "y = train_df['target']\n",
    "train_df.drop('target', axis=1, inplace=True)\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "train_df[times] = train_df[times].apply(pd.to_datetime)\n",
    "test_df[times] = test_df[times].apply(pd.to_datetime)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всего сайтов: 48371\n",
      "            site1               time1  site2               time2    site3  \\\n",
      "session_id                                                                  \n",
      "1             718 2014-02-20 10:02:45    NaN                 NaT      NaN   \n",
      "2             890 2014-02-22 11:19:50  941.0 2014-02-22 11:19:50   3847.0   \n",
      "3           14769 2013-12-16 16:40:17   39.0 2013-12-16 16:40:18  14768.0   \n",
      "4             782 2014-03-28 10:52:12  782.0 2014-03-28 10:52:42    782.0   \n",
      "5              22 2014-02-28 10:53:05  177.0 2014-02-28 10:55:22    175.0   \n",
      "\n",
      "                         time3    site4               time4  site5  \\\n",
      "session_id                                                           \n",
      "1                          NaT      NaN                 NaT    NaN   \n",
      "2          2014-02-22 11:19:51    941.0 2014-02-22 11:19:51  942.0   \n",
      "3          2013-12-16 16:40:19  14769.0 2013-12-16 16:40:19   37.0   \n",
      "4          2014-03-28 10:53:12    782.0 2014-03-28 10:53:42  782.0   \n",
      "5          2014-02-28 10:55:22    178.0 2014-02-28 10:55:23  177.0   \n",
      "\n",
      "                         time5         ...                      site_column1  \\\n",
      "session_id                             ...                                     \n",
      "1                          NaT         ...           rr.office.microsoft.com   \n",
      "2          2014-02-22 11:19:51         ...                   maps.google.com   \n",
      "3          2013-12-16 16:40:19         ...               cbk1.googleapis.com   \n",
      "4          2014-03-28 10:54:12         ...                    annotathon.org   \n",
      "5          2014-02-28 10:55:23         ...                   apis.google.com   \n",
      "\n",
      "                   site_column2         site_column3         site_column4  \\\n",
      "session_id                                                                  \n",
      "1                           NaN                  NaN                  NaN   \n",
      "2               mts0.google.com     khms0.google.com      mts0.google.com   \n",
      "3           accounts.google.com  cbk0.googleapis.com  cbk1.googleapis.com   \n",
      "4                annotathon.org       annotathon.org       annotathon.org   \n",
      "5              fr.wikipedia.org   bits.wikimedia.org   meta.wikimedia.org   \n",
      "\n",
      "                site_column5         site_column6         site_column7  \\\n",
      "session_id                                                               \n",
      "1                        NaN                  NaN                  NaN   \n",
      "2            mts1.google.com     khms1.google.com     khms0.google.com   \n",
      "3                twitter.com  accounts.google.com  cbk0.googleapis.com   \n",
      "4             annotathon.org       annotathon.org       annotathon.org   \n",
      "5           fr.wikipedia.org   meta.wikimedia.org   bits.wikimedia.org   \n",
      "\n",
      "                   site_column8         site_column9        site_column10  \n",
      "session_id                                                                 \n",
      "1                           NaN                  NaN                  NaN  \n",
      "2              khms1.google.com       193.164.197.30       193.164.196.60  \n",
      "3           cbk0.googleapis.com  cbk0.googleapis.com  cbk0.googleapis.com  \n",
      "4                annotathon.org       annotathon.org       annotathon.org  \n",
      "5              fr.wikipedia.org     fr.wikipedia.org   meta.wikimedia.org  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=100000, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загрузим словарик сайтов\n",
    "with open(r\"/home/andrei/Desktop/alice_kaggle/site_dic.pkl\", \"rb\") as input_file:\n",
    "    site_dict = pickle.load(input_file)\n",
    "\n",
    "# датафрейм словарика сайтов\n",
    "sites_dict_df = pd.DataFrame(list(site_dict.keys()), \n",
    "                          index=list(site_dict.values()), \n",
    "                          columns=['site'])\n",
    "print(u'всего сайтов:', sites_dict_df.shape[0])\n",
    "\n",
    "text_columns = ['site_column%s' % i for i in range(1, 11)]\n",
    "train_df['text_col'] = ''\n",
    "test_df['text_col'] = ''\n",
    "\n",
    "for i in range(1, 11):\n",
    "    site_c = 'site{}'.format(i)\n",
    "    site_name_c = 'site_column{}'.format(i)\n",
    "    train_df[site_name_c] = sites_dict_df.loc[train_df[site_c]].values\n",
    "    test_df[site_name_c] = sites_dict_df.loc[test_df[site_c]].values\n",
    "    train_df['text_col'] += train_df[site_name_c]\n",
    "    test_df['text_col'] += test_df[site_name_c]\n",
    "    \n",
    "print(train_df.head())\n",
    "\n",
    "all_train_text = pd.concat([train_df['site_column{}'.format(i)].astype('U') for i in range(1, 11)])  \n",
    "char_vec = TfidfVectorizer(analyzer='char', ngram_range=(1, 3), max_features=100000)\n",
    "word_vec = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), max_features=100000)\n",
    "# fix wordofvec\n",
    "word_vec.fit(all_train_text.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = word_vec.transform(pd.concat((train_df['text_col'], test_df['text_col'])).values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_common = dict()\n",
    "for i in range(1, 11):\n",
    "    a = train_df[y == 0]['site_column%s' % i].value_counts()\n",
    "    for k, v in dict(a).items():\n",
    "        alice_common.setdefault(k, 0)\n",
    "        alice_common[k] += v\n",
    "alice_common = list(alice_common.items())\n",
    "alice_common.sort(key=lambda x: x[1], reverse=True)\n",
    "alice_common = set([_[0] for _ in alice_common[:30]])\n",
    "\n",
    "\n",
    "def get_part_from_hour(h):\n",
    "    if h < 6:\n",
    "        return 0\n",
    "    if h < 12:\n",
    "        return 1\n",
    "    if h < 18:\n",
    "        return 2\n",
    "    return 3\n",
    "\n",
    "\n",
    "def get_session_duration(row):\n",
    "    time_values = row[['time%s' % i for i in range(1, 10)]].dropna().values\n",
    "    duration = (time_values.max() - time_values.min()).total_seconds()\n",
    "    return duration\n",
    "\n",
    "\n",
    "def get_count_of_good_sites(row):\n",
    "    res = 0\n",
    "    for i in range(1, 11):\n",
    "        res += row['site_column%s' % i] in alice_common\n",
    "    return res\n",
    "\n",
    "\n",
    "def add_extra_features(df):\n",
    "    df['hour'] = df['time1'].dt.hour\n",
    "    \n",
    "    for i in range(2, 4):\n",
    "        df['delta%s' % (i-1)] = (df['time%s' % i] - df['time%s' % (i-1)]).dt.total_seconds()\n",
    "\n",
    "    #df['duration'] = df.apply(get_session_duration, axis=1)\n",
    "    \n",
    "    df['count_good'] = df.apply(get_count_of_good_sites, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_scaled_features(df):\n",
    "    scalable_columns = ['hour', 'delta1',  'count_good'] # 'duration']\n",
    "    return MinMaxScaler().fit_transform(df[scalable_columns].fillna(-1))\n",
    "\n",
    "\n",
    "def get_part_of_day(df):\n",
    "    df['part_of_day'] = np.array(list(map(lambda v: get_part_from_hour(v), df['time1'].dt.hour)))\n",
    "    return OneHotEncoder().fit_transform(df[['part_of_day']])\n",
    "\n",
    "\n",
    "def get_work_time(df):\n",
    "    df['hour'] = df['time1'].dt.hour\n",
    "    df['is_work_time'] = df['hour'].apply(lambda x: 8 <= x <= 17)\n",
    "    return OneHotEncoder().fit_transform(df[['is_work_time']])\n",
    "\n",
    "\n",
    "def get_extra_features(df):\n",
    "    return get_scaled_features(add_extra_features(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc_lr_valid(X, y, C=1.0, seed=17, ratio = 0.9):\n",
    "    idx = int(round(X.shape[0] * ratio))\n",
    "    lr = LogisticRegression(C=C, random_state=seed, n_jobs=-1).fit(X[:idx, :], y[:idx])\n",
    "    y_pred = lr.predict_proba(X[idx:, :])[:, 1]\n",
    "    score = roc_auc_score(y[idx:], y_pred)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence of indices\n",
    "sites_flatten = full_sites.values.flatten()\n",
    "\n",
    "# and the matrix we are looking for\n",
    "full_sites_sparse = csr_matrix(([1] * sites_flatten.shape[0],\n",
    "                                sites_flatten,\n",
    "                                range(0, sites_flatten.shape[0]  + 10, 10)))[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>941.0</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>941.0</td>\n",
       "      <td>942.0</td>\n",
       "      <td>3846.0</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>3846.0</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>1518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>39.0</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>14769.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>14768.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>782.0</td>\n",
       "      <td>782.0</td>\n",
       "      <td>782.0</td>\n",
       "      <td>782.0</td>\n",
       "      <td>782.0</td>\n",
       "      <td>782.0</td>\n",
       "      <td>782.0</td>\n",
       "      <td>782.0</td>\n",
       "      <td>782.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>177.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2    site3    site4  site5   site6    site7    site8  \\\n",
       "session_id                                                                    \n",
       "1             718    NaN      NaN      NaN    NaN     NaN      NaN      NaN   \n",
       "2             890  941.0   3847.0    941.0  942.0  3846.0   3847.0   3846.0   \n",
       "3           14769   39.0  14768.0  14769.0   37.0    39.0  14768.0  14768.0   \n",
       "4             782  782.0    782.0    782.0  782.0   782.0    782.0    782.0   \n",
       "5              22  177.0    175.0    178.0  177.0   178.0    175.0    177.0   \n",
       "\n",
       "              site9   site10  \n",
       "session_id                    \n",
       "1               NaN      NaN  \n",
       "2            1516.0   1518.0  \n",
       "3           14768.0  14768.0  \n",
       "4             782.0    782.0  \n",
       "5             177.0    178.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = pd.concat([train_df, test_df])\n",
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "# Index to split the training and test data sets\n",
    "idx_split = train_df.shape[0]\n",
    "full_sites = full_df[sites]\n",
    "full_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336358, 48371)\n",
      "(336358, 100000)\n",
      "(336358, 31)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_train = pd.concat((train_df, test_df))\n",
    "#X_train = full_sites_sparse[:idx_split, :]\n",
    "print(full_sites_sparse.shape)\n",
    "print(X_tfidf.shape)\n",
    "print(_train.shape)\n",
    "X_train = hstack([full_sites_sparse,# X_tfidf,\n",
    "                  get_part_of_day(_train), \n",
    "                  get_work_time(_train),\n",
    "                  get_extra_features(_train)\n",
    "                 ], format='csr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979840797432511\n"
     ]
    }
   ],
   "source": [
    "# Calculate metric on the validation set\n",
    "print(get_auc_lr_valid(X_train[:idx_split], y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for writing predictions to a file\n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=1, random_state=17, n_jobs=-1).fit(X_train[:idx_split], y)\n",
    "X_test = X_train[idx_split:,:]\n",
    "y_test = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Write it to the file which could be submitted\n",
    "write_to_submission_file(y_test, 'alice.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
