{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inpired by https://github.com/aerdem4/kaggle-quora-dup\n",
    "# useful: https://habrahabr.ru/company/dca/blog/274027/\n",
    "# link to the contest: https://contest.yandex.ru/algorithm2018/contest/7914\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import csv\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import concatenate, add, multiply\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 753 ms, sys: 24.8 ms, total: 778 ms\n",
      "Wall time: 778 ms\n"
     ]
    }
   ],
   "source": [
    "train_columns = \"context_id,context_2,context_1,context_0,reply_id,reply,label,confidence\".split(',')\n",
    "train = pd.read_csv(\"/home/andrei/yandex_ml_track/train.tsv\", delimiter='\\t', names=train_columns, quoting=csv.QUOTE_NONE)\n",
    "train = train.fillna('')\n",
    "\n",
    "test_columns = \"context_id,context_2,context_1,context_0,reply_id,reply\".split(',')\n",
    "test = pd.read_csv(\"/home/andrei/yandex_ml_track/final.tsv\", delimiter='\\t', names=test_columns, quoting=csv.QUOTE_NONE)\n",
    "test = test.fillna('')\n",
    "\n",
    "res = {'good': 1, 'neutral': 0.1, 'bad': 0}\n",
    "y = train['label'].map(res)\n",
    "\n",
    "fx = lambda x: x >= 0.85\n",
    "\n",
    "y = y*(train['confidence'].apply(fx))  # very important\n",
    "train.drop('label', axis=1)\n",
    "\n",
    "text_columns = ['context_2', 'context_1', 'context_0', 'reply']\n",
    "train['text'] = test['text'] = ''\n",
    "for c in text_columns[:-1]:\n",
    "    train['text'] += train[c].astype('U')\n",
    "    test['text'] += test[c].astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(result, out_file):\n",
    "    predicted_df = pd.DataFrame(result[['context_id', 'reply_id']],\n",
    "                                columns=['context_id', 'reply_id'])\n",
    "    predicted_df.to_csv(out_file, index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 s, sys: 887 ms, total: 20.8 s\n",
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "# w2v related stuff\n",
    "PATH_W2VEC = '/home/andrei/Downloads/all.norm-sz100-w10-cb0-it1-min100.w2v'\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(PATH_W2VEC, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mean_vectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "_w2v = dict(zip(w2v.index2word, w2v.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-324-8cc1e9323484>, line 50)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-324-8cc1e9323484>\"\u001b[0;36m, line \u001b[0;32m50\u001b[0m\n\u001b[0;31m    last_word_text, last_word_reply]) #, words0, words1])\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "vectorizer = mean_vectorizer(_w2v)\n",
    "\n",
    "np.random.seed(0)\n",
    "WNL = SnowballStemmer(\"russian\")\n",
    "STOP_WORDS = set(stopwords.words('russian'))\n",
    "MAX_SEQUENCE_LENGTH = 35\n",
    "MIN_WORD_OCCURRENCE = 100\n",
    "REPLACE_WORD = \"ПУСТОТА\"\n",
    "EMBEDDING_DIM = 100\n",
    "NUM_FOLDS = 10\n",
    "BATCH_SIZE = 1025\n",
    "\n",
    "\n",
    "def words_cutter(word):\n",
    "    if len(word) < 4:\n",
    "        return word\n",
    "    return WNL.stem(word)\n",
    "\n",
    "\n",
    "def preprocess(s):\n",
    "    for c in '[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]':\n",
    "        s = s.replace(c, ' {} '.format(c))\n",
    "    \n",
    "    s = ' '.join(s.split())\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "def is_numeric(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "\n",
    "def prepare(q):\n",
    "    new_q = []\n",
    "    surplus_q = []\n",
    "    numbers_q = []\n",
    "    new_memento = True\n",
    "    for w in q.split()[::-1]:\n",
    "        if w in top_words:\n",
    "            new_q = [w] + new_q\n",
    "            new_memento = True\n",
    "        elif w not in STOP_WORDS:\n",
    "            if new_memento:\n",
    "                new_q = [REPLACE_WORD] + new_q\n",
    "                new_memento = False\n",
    "            if is_numeric(w):\n",
    "                numbers_q = [w] + numbers_q\n",
    "            else:\n",
    "                surplus_q = [w] + surplus_q\n",
    "        else:\n",
    "            new_memento = True\n",
    "        if len(new_q) == MAX_SEQUENCE_LENGTH:\n",
    "            break\n",
    "    new_q = \" \".join(new_q)\n",
    "    return new_q, set(surplus_q), set(numbers_q)\n",
    "\n",
    "def get_special_symbols_count(text):\n",
    "    res = 0\n",
    "    for c in '“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@':\n",
    "        res += text.count(c)\n",
    "    return res\n",
    "\n",
    "def get_first_word(text):\n",
    "    text = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', text).split()\n",
    "    return text[0] if text else ''\n",
    "\n",
    "def get_last_word(text):\n",
    "    text = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', text)\n",
    "    return text[-1] if text else ''\n",
    "\n",
    "def get_word_count(text):\n",
    "    text = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', text)\n",
    "    return len(text.split())\n",
    "\n",
    "def get_features(dataset):\n",
    "    cnt0 = dataset['text'].apply(get_special_symbols_count).reshape(len(dataset), 1)\n",
    "    cnt1 = dataset['reply'].apply(get_special_symbols_count).reshape(len(dataset), 1)\n",
    "    \n",
    "    dataset['first_text_word'] = dataset['text'].apply(get_first_word)\n",
    "    dataset['first_reply_word'] = dataset['reply'].apply(get_first_word)\n",
    "    \n",
    "    dataset['last_text_word'] = dataset['text'].apply(get_last_word)\n",
    "    dataset['last_reply_word'] = dataset['reply'].apply(get_last_word)\n",
    "    \n",
    "    reply_columns = vectorizer.transform(dataset['text'])\n",
    "    text_columns = vectorizer.transform(dataset['reply'])\n",
    "\n",
    "    length0 = dataset['text'].apply(len).reshape(len(dataset), 1)\n",
    "    length1 = dataset['reply'].apply(len).reshape(len(dataset), 1)\n",
    "    words0 = dataset['text'].apply(get_word_count).reshape(len(dataset), 1)\n",
    "    words1 = dataset['text'].apply(get_word_count).reshape(len(dataset), 1)\n",
    "    \n",
    "    first_word_text = vectorizer.transform(dataset['first_text_word'])\n",
    "    last_word_text = vectorizer.transform(dataset['last_text_word'])\n",
    "    \n",
    "    first_word_reply = vectorizer.transform(dataset['first_reply_word'])\n",
    "    last_word_reply = vectorizer.transform(dataset['last_reply_word'])\n",
    "    \n",
    "    print(length0.shape, reply_columns.shape)\n",
    "    df = np.hstack([text_columns, reply_columns, length0, length1, cnt0, cnt1,\n",
    "                    #first_word_text, first_word_reply, \n",
    "                    last_word_text, last_word_reply]) #, words0, words1])\n",
    "    print('Shape: ', df.shape)\n",
    "    return df\n",
    "\n",
    "def extract_features(df):\n",
    "    q1s = np.array([\"\"] * len(df), dtype=object)\n",
    "    q2s = np.array([\"\"] * len(df), dtype=object)\n",
    "    features = np.zeros((len(df), 4))\n",
    "\n",
    "    for i, (q1, q2) in enumerate(list(zip(df[\"text\"], df[\"reply\"]))):\n",
    "        q1s[i], surplus1, numbers1 = prepare(q1)\n",
    "        q2s[i], surplus2, numbers2 = prepare(q2)\n",
    "        features[i, 0] = len(surplus1.intersection(surplus2))\n",
    "        features[i, 1] = len(surplus1.union(surplus2))\n",
    "        features[i, 2] = len(numbers1.intersection(numbers2))\n",
    "        features[i, 3] = len(numbers1.union(numbers2))\n",
    "    \n",
    "        \n",
    "    w2v_mean_vec = get_features(df)\n",
    "    \n",
    "    return q1s, q2s, np.hstack([features, w2v_mean_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/.local/lib/python3.5/site-packages/ipykernel_launcher.py:24: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/andrei/.local/lib/python3.5/site-packages/ipykernel_launcher.py:25: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/andrei/.local/lib/python3.5/site-packages/ipykernel_launcher.py:36: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/andrei/.local/lib/python3.5/site-packages/ipykernel_launcher.py:37: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/andrei/.local/lib/python3.5/site-packages/ipykernel_launcher.py:38: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/andrei/.local/lib/python3.5/site-packages/ipykernel_launcher.py:39: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97533, 1) (97533, 100)\n",
      "Shape:  (97533, 204)\n",
      "(68273, 35) (29260, 35) (68273, 35) (29260, 35) (68273,) (29260,) 0.31027931978966794 0.31027931978966794\n",
      "(97533, 208)\n"
     ]
    }
   ],
   "source": [
    "t_train, r_train, f = extract_features(train)\n",
    "\n",
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(np.append(t_train, r_train))\n",
    "data_1 = pad_sequences(tokenizer.texts_to_sequences(t_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(tokenizer.texts_to_sequences(r_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "x1_tr, x1_val, y_tr, y_val = split(data_1, y, 0.7)\n",
    "x2_tr, x2_val, y_tr, y_val = split(data_2, y, 0.7)\n",
    "f_tr, f_val, y_tr, y_val = split(f, y, 0.7)\n",
    "print(x1_tr.shape, x1_val.shape, x2_tr.shape, x2_val.shape, y_tr.shape, y_val.shape, y_tr.mean(), y_tr.mean())\n",
    "print(f.shape)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = _w2v.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "top_words = _w2v.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1\n",
    "# model = Sequential()\n",
    "# model.add(Dense(128, input_dim=(Xtr.shape[1])))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='nadam',\n",
    "#               metrics=['mse'])\n",
    "\n",
    "\n",
    "#Model 2\n",
    "# model = Sequential()\n",
    "# model.add(Dense(128, input_dim=(Xtr.shape[1])))\n",
    "# #model.add(BatchNormalization(batch_input_shape=(Xtr.shape[1],)))\n",
    "# model.add(GaussianNoise(0.1))\n",
    "# model.add(Dense(150, activation=\"relu\"))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='nadam',\n",
    "#               metrics=['mse'])\n",
    "\n",
    "\n",
    "# history = model.fit(Xtr, ytr, \n",
    "#                     batch_size=128,\n",
    "#                     epochs=50,\n",
    "#                     validation_data=(Xval, yval),\n",
    "#                     class_weight='auto',\n",
    "#                     verbose=1)\n",
    "\n",
    "embedding_layer = Embedding(nb_words,\n",
    "          EMBEDDING_DIM,\n",
    "          weights=[embedding_matrix],\n",
    "          input_length=MAX_SEQUENCE_LENGTH,\n",
    "          trainable=False)\n",
    "\n",
    "lstm_layer = LSTM(75, recurrent_dropout=0.2)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "addition = add([x1, y1])\n",
    "minus_y1 = Lambda(lambda x: -x)(y1)\n",
    "merged = add([x1, minus_y1])\n",
    "merged = multiply([merged, merged])\n",
    "merged = concatenate([merged, addition])\n",
    "\n",
    "\n",
    "features_input = Input(shape=(f.shape[1],), dtype=\"float32\")\n",
    "features_dense = BatchNormalization()(features_input)\n",
    "features_dense = Dense(200, activation=\"relu\")(features_dense)\n",
    "features_dense = Dropout(0.2)(features_dense)\n",
    "\n",
    "merged = concatenate([merged, features_dense])\n",
    "\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = GaussianNoise(0.1)(merged)\n",
    "merged = Dense(150, activation=\"relu\")(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(1)(merged)\n",
    "merged = Activation('sigmoid')(merged)\n",
    "\n",
    "out = Dropout(0.4)(merged)\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input, features_input]\n",
    "              , outputs=out)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', #'mean_squared_error'\n",
    "              optimizer='nadam',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68273 samples, validate on 29260 samples\n",
      "Epoch 1/300\n",
      "68273/68273 [==============================] - 57s 836us/step - loss: 3.6642 - mean_squared_error: 0.4188 - val_loss: 0.8067 - val_mean_squared_error: 0.2410\n",
      "Epoch 2/300\n",
      "68273/68273 [==============================] - 52s 767us/step - loss: 2.4938 - mean_squared_error: 0.2703 - val_loss: 0.6703 - val_mean_squared_error: 0.2171\n",
      "Epoch 3/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.4059 - mean_squared_error: 0.2536 - val_loss: 0.6977 - val_mean_squared_error: 0.2222\n",
      "Epoch 4/300\n",
      "68273/68273 [==============================] - 52s 765us/step - loss: 2.4120 - mean_squared_error: 0.2521 - val_loss: 0.6522 - val_mean_squared_error: 0.2173\n",
      "Epoch 5/300\n",
      "68273/68273 [==============================] - 52s 766us/step - loss: 2.3858 - mean_squared_error: 0.2484 - val_loss: 0.6466 - val_mean_squared_error: 0.2172\n",
      "Epoch 6/300\n",
      "68273/68273 [==============================] - 54s 793us/step - loss: 2.3880 - mean_squared_error: 0.2485 - val_loss: 0.6528 - val_mean_squared_error: 0.2180\n",
      "Epoch 7/300\n",
      "68273/68273 [==============================] - 52s 768us/step - loss: 2.3968 - mean_squared_error: 0.2487 - val_loss: 0.6661 - val_mean_squared_error: 0.2195\n",
      "Epoch 8/300\n",
      "68273/68273 [==============================] - 53s 769us/step - loss: 2.3775 - mean_squared_error: 0.2452 - val_loss: 0.6545 - val_mean_squared_error: 0.2183\n",
      "Epoch 9/300\n",
      "68273/68273 [==============================] - 52s 768us/step - loss: 2.3376 - mean_squared_error: 0.2428 - val_loss: 0.6491 - val_mean_squared_error: 0.2169\n",
      "Epoch 10/300\n",
      "68273/68273 [==============================] - 52s 767us/step - loss: 2.3788 - mean_squared_error: 0.2437 - val_loss: 0.6485 - val_mean_squared_error: 0.2172\n",
      "Epoch 11/300\n",
      "68273/68273 [==============================] - 52s 765us/step - loss: 2.3620 - mean_squared_error: 0.2425 - val_loss: 0.6401 - val_mean_squared_error: 0.2153\n",
      "Epoch 12/300\n",
      "68273/68273 [==============================] - 52s 767us/step - loss: 2.3652 - mean_squared_error: 0.2420 - val_loss: 0.6442 - val_mean_squared_error: 0.2159\n",
      "Epoch 13/300\n",
      "68273/68273 [==============================] - 52s 766us/step - loss: 2.3518 - mean_squared_error: 0.2416 - val_loss: 0.6422 - val_mean_squared_error: 0.2151\n",
      "Epoch 14/300\n",
      "68273/68273 [==============================] - 52s 768us/step - loss: 2.3682 - mean_squared_error: 0.2414 - val_loss: 0.6432 - val_mean_squared_error: 0.2160\n",
      "Epoch 15/300\n",
      "68273/68273 [==============================] - 53s 769us/step - loss: 2.3501 - mean_squared_error: 0.2405 - val_loss: 0.6328 - val_mean_squared_error: 0.2127\n",
      "Epoch 16/300\n",
      "68273/68273 [==============================] - 53s 769us/step - loss: 2.3549 - mean_squared_error: 0.2408 - val_loss: 0.6446 - val_mean_squared_error: 0.2166\n",
      "Epoch 17/300\n",
      "68273/68273 [==============================] - 52s 769us/step - loss: 2.3445 - mean_squared_error: 0.2398 - val_loss: 0.6472 - val_mean_squared_error: 0.2171\n",
      "Epoch 18/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3707 - mean_squared_error: 0.2406 - val_loss: 0.6370 - val_mean_squared_error: 0.2138\n",
      "Epoch 19/300\n",
      "68273/68273 [==============================] - 53s 780us/step - loss: 2.3541 - mean_squared_error: 0.2397 - val_loss: 0.6468 - val_mean_squared_error: 0.2169\n",
      "Epoch 20/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3627 - mean_squared_error: 0.2395 - val_loss: 0.6302 - val_mean_squared_error: 0.2116\n",
      "Epoch 21/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.3509 - mean_squared_error: 0.2387 - val_loss: 0.6315 - val_mean_squared_error: 0.2123\n",
      "Epoch 22/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.3676 - mean_squared_error: 0.2396 - val_loss: 0.6369 - val_mean_squared_error: 0.2139\n",
      "Epoch 23/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.3411 - mean_squared_error: 0.2377 - val_loss: 0.6409 - val_mean_squared_error: 0.2152\n",
      "Epoch 24/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3714 - mean_squared_error: 0.2390 - val_loss: 0.6351 - val_mean_squared_error: 0.2131\n",
      "Epoch 25/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3735 - mean_squared_error: 0.2394 - val_loss: 0.6370 - val_mean_squared_error: 0.2143\n",
      "Epoch 26/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3368 - mean_squared_error: 0.2366 - val_loss: 0.6322 - val_mean_squared_error: 0.2119\n",
      "Epoch 27/300\n",
      "68273/68273 [==============================] - 52s 768us/step - loss: 2.3305 - mean_squared_error: 0.2365 - val_loss: 0.6383 - val_mean_squared_error: 0.2141\n",
      "Epoch 28/300\n",
      "68273/68273 [==============================] - 52s 769us/step - loss: 2.3605 - mean_squared_error: 0.2369 - val_loss: 0.6389 - val_mean_squared_error: 0.2141\n",
      "Epoch 29/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3876 - mean_squared_error: 0.2374 - val_loss: 0.6634 - val_mean_squared_error: 0.2211\n",
      "Epoch 30/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.3289 - mean_squared_error: 0.2351 - val_loss: 0.6285 - val_mean_squared_error: 0.2111\n",
      "Epoch 31/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3420 - mean_squared_error: 0.2354 - val_loss: 0.6472 - val_mean_squared_error: 0.2171\n",
      "Epoch 32/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.3573 - mean_squared_error: 0.2370 - val_loss: 0.6470 - val_mean_squared_error: 0.2163\n",
      "Epoch 33/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3630 - mean_squared_error: 0.2363 - val_loss: 0.6310 - val_mean_squared_error: 0.2114\n",
      "Epoch 34/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.3183 - mean_squared_error: 0.2344 - val_loss: 0.6314 - val_mean_squared_error: 0.2117\n",
      "Epoch 35/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.3452 - mean_squared_error: 0.2349 - val_loss: 0.6235 - val_mean_squared_error: 0.2092\n",
      "Epoch 36/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3323 - mean_squared_error: 0.2336 - val_loss: 0.6219 - val_mean_squared_error: 0.2079\n",
      "Epoch 37/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.3213 - mean_squared_error: 0.2322 - val_loss: 0.6293 - val_mean_squared_error: 0.2110\n",
      "Epoch 38/300\n",
      "68273/68273 [==============================] - 53s 769us/step - loss: 2.3241 - mean_squared_error: 0.2324 - val_loss: 0.6381 - val_mean_squared_error: 0.2128\n",
      "Epoch 39/300\n",
      "68273/68273 [==============================] - 52s 768us/step - loss: 2.3261 - mean_squared_error: 0.2320 - val_loss: 0.6479 - val_mean_squared_error: 0.2151\n",
      "Epoch 40/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.3293 - mean_squared_error: 0.2328 - val_loss: 0.6269 - val_mean_squared_error: 0.2103\n",
      "Epoch 41/300\n",
      "68273/68273 [==============================] - 52s 768us/step - loss: 2.3391 - mean_squared_error: 0.2327 - val_loss: 0.6262 - val_mean_squared_error: 0.2103\n",
      "Epoch 42/300\n",
      "68273/68273 [==============================] - 53s 769us/step - loss: 2.3248 - mean_squared_error: 0.2320 - val_loss: 0.6375 - val_mean_squared_error: 0.2132\n",
      "Epoch 43/300\n",
      "68273/68273 [==============================] - 52s 768us/step - loss: 2.3203 - mean_squared_error: 0.2302 - val_loss: 0.6464 - val_mean_squared_error: 0.2150\n",
      "Epoch 44/300\n",
      "68273/68273 [==============================] - 52s 767us/step - loss: 2.3012 - mean_squared_error: 0.2289 - val_loss: 0.6566 - val_mean_squared_error: 0.2184\n",
      "Epoch 45/300\n",
      "68273/68273 [==============================] - 52s 767us/step - loss: 2.3339 - mean_squared_error: 0.2313 - val_loss: 0.6291 - val_mean_squared_error: 0.2107\n",
      "Epoch 46/300\n",
      "68273/68273 [==============================] - 52s 769us/step - loss: 2.3210 - mean_squared_error: 0.2297 - val_loss: 0.6380 - val_mean_squared_error: 0.2127\n",
      "Epoch 47/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3001 - mean_squared_error: 0.2274 - val_loss: 0.6474 - val_mean_squared_error: 0.2153\n",
      "Epoch 48/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.3347 - mean_squared_error: 0.2340 - val_loss: 0.6144 - val_mean_squared_error: 0.2056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300\n",
      "68273/68273 [==============================] - 52s 768us/step - loss: 2.3535 - mean_squared_error: 0.2349 - val_loss: 0.6299 - val_mean_squared_error: 0.2095\n",
      "Epoch 50/300\n",
      "68273/68273 [==============================] - 52s 767us/step - loss: 2.3459 - mean_squared_error: 0.2319 - val_loss: 0.6360 - val_mean_squared_error: 0.2116\n",
      "Epoch 51/300\n",
      "68273/68273 [==============================] - 52s 768us/step - loss: 2.3449 - mean_squared_error: 0.2330 - val_loss: 0.6506 - val_mean_squared_error: 0.2160\n",
      "Epoch 52/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.3652 - mean_squared_error: 0.2334 - val_loss: 0.6862 - val_mean_squared_error: 0.2244\n",
      "Epoch 53/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3004 - mean_squared_error: 0.2284 - val_loss: 0.6521 - val_mean_squared_error: 0.2165\n",
      "Epoch 54/300\n",
      "68273/68273 [==============================] - 53s 780us/step - loss: 2.3270 - mean_squared_error: 0.2305 - val_loss: 0.6657 - val_mean_squared_error: 0.2199\n",
      "Epoch 55/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3103 - mean_squared_error: 0.2279 - val_loss: 0.6465 - val_mean_squared_error: 0.2146\n",
      "Epoch 56/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.3312 - mean_squared_error: 0.2284 - val_loss: 0.6531 - val_mean_squared_error: 0.2148\n",
      "Epoch 57/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.3096 - mean_squared_error: 0.2267 - val_loss: 0.6297 - val_mean_squared_error: 0.2096\n",
      "Epoch 58/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.3133 - mean_squared_error: 0.2264 - val_loss: 0.6367 - val_mean_squared_error: 0.2118\n",
      "Epoch 59/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.3328 - mean_squared_error: 0.2264 - val_loss: 0.6620 - val_mean_squared_error: 0.2173\n",
      "Epoch 60/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.3253 - mean_squared_error: 0.2247 - val_loss: 0.6419 - val_mean_squared_error: 0.2119\n",
      "Epoch 61/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3030 - mean_squared_error: 0.2232 - val_loss: 0.6283 - val_mean_squared_error: 0.2072\n",
      "Epoch 62/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3286 - mean_squared_error: 0.2255 - val_loss: 0.6802 - val_mean_squared_error: 0.2205\n",
      "Epoch 63/300\n",
      "68273/68273 [==============================] - 52s 769us/step - loss: 2.3217 - mean_squared_error: 0.2238 - val_loss: 0.7128 - val_mean_squared_error: 0.2277\n",
      "Epoch 64/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3341 - mean_squared_error: 0.2343 - val_loss: 0.6070 - val_mean_squared_error: 0.2026\n",
      "Epoch 65/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.3232 - mean_squared_error: 0.2336 - val_loss: 0.6340 - val_mean_squared_error: 0.2104\n",
      "Epoch 66/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3438 - mean_squared_error: 0.2323 - val_loss: 0.6803 - val_mean_squared_error: 0.2227\n",
      "Epoch 67/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.3010 - mean_squared_error: 0.2274 - val_loss: 0.6851 - val_mean_squared_error: 0.2236\n",
      "Epoch 68/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.3158 - mean_squared_error: 0.2269 - val_loss: 0.6674 - val_mean_squared_error: 0.2188\n",
      "Epoch 69/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.2997 - mean_squared_error: 0.2249 - val_loss: 0.6560 - val_mean_squared_error: 0.2159\n",
      "Epoch 70/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.3060 - mean_squared_error: 0.2238 - val_loss: 0.6526 - val_mean_squared_error: 0.2145\n",
      "Epoch 71/300\n",
      "68273/68273 [==============================] - 52s 769us/step - loss: 2.3042 - mean_squared_error: 0.2223 - val_loss: 0.6629 - val_mean_squared_error: 0.2164\n",
      "Epoch 72/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.3361 - mean_squared_error: 0.2232 - val_loss: 0.6356 - val_mean_squared_error: 0.2109\n",
      "Epoch 73/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2962 - mean_squared_error: 0.2229 - val_loss: 0.8209 - val_mean_squared_error: 0.2362\n",
      "Epoch 74/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.3165 - mean_squared_error: 0.2297 - val_loss: 0.6594 - val_mean_squared_error: 0.2157\n",
      "Epoch 75/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.3161 - mean_squared_error: 0.2247 - val_loss: 0.6597 - val_mean_squared_error: 0.2161\n",
      "Epoch 76/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.3043 - mean_squared_error: 0.2221 - val_loss: 0.6506 - val_mean_squared_error: 0.2130\n",
      "Epoch 77/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.2872 - mean_squared_error: 0.2224 - val_loss: 0.6330 - val_mean_squared_error: 0.2092\n",
      "Epoch 78/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2866 - mean_squared_error: 0.2204 - val_loss: 0.6635 - val_mean_squared_error: 0.2154\n",
      "Epoch 79/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.3049 - mean_squared_error: 0.2198 - val_loss: 0.6894 - val_mean_squared_error: 0.2195\n",
      "Epoch 80/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.3023 - mean_squared_error: 0.2203 - val_loss: 0.6883 - val_mean_squared_error: 0.2198\n",
      "Epoch 81/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2859 - mean_squared_error: 0.2201 - val_loss: 0.6683 - val_mean_squared_error: 0.2154\n",
      "Epoch 82/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2963 - mean_squared_error: 0.2187 - val_loss: 0.7066 - val_mean_squared_error: 0.2238\n",
      "Epoch 83/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.3053 - mean_squared_error: 0.2186 - val_loss: 0.6737 - val_mean_squared_error: 0.2171\n",
      "Epoch 84/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.3001 - mean_squared_error: 0.2169 - val_loss: 0.6747 - val_mean_squared_error: 0.2155\n",
      "Epoch 85/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2882 - mean_squared_error: 0.2225 - val_loss: 0.6087 - val_mean_squared_error: 0.2032\n",
      "Epoch 86/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.3099 - mean_squared_error: 0.2247 - val_loss: 0.6873 - val_mean_squared_error: 0.2189\n",
      "Epoch 87/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2991 - mean_squared_error: 0.2207 - val_loss: 0.6878 - val_mean_squared_error: 0.2197\n",
      "Epoch 88/300\n",
      "68273/68273 [==============================] - 53s 781us/step - loss: 2.3065 - mean_squared_error: 0.2220 - val_loss: 0.7086 - val_mean_squared_error: 0.2240\n",
      "Epoch 89/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2956 - mean_squared_error: 0.2206 - val_loss: 0.6389 - val_mean_squared_error: 0.2073\n",
      "Epoch 90/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.3178 - mean_squared_error: 0.2264 - val_loss: 0.6437 - val_mean_squared_error: 0.2113\n",
      "Epoch 91/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.3231 - mean_squared_error: 0.2210 - val_loss: 0.6836 - val_mean_squared_error: 0.2181\n",
      "Epoch 92/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.2748 - mean_squared_error: 0.2174 - val_loss: 0.6599 - val_mean_squared_error: 0.2141\n",
      "Epoch 93/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2752 - mean_squared_error: 0.2161 - val_loss: 0.6849 - val_mean_squared_error: 0.2167\n",
      "Epoch 94/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2959 - mean_squared_error: 0.2159 - val_loss: 0.7170 - val_mean_squared_error: 0.2240\n",
      "Epoch 95/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2759 - mean_squared_error: 0.2163 - val_loss: 0.6966 - val_mean_squared_error: 0.2196\n",
      "Epoch 96/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.3212 - mean_squared_error: 0.2159 - val_loss: 0.6774 - val_mean_squared_error: 0.2151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.2986 - mean_squared_error: 0.2170 - val_loss: 0.6295 - val_mean_squared_error: 0.2117\n",
      "Epoch 98/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2992 - mean_squared_error: 0.2215 - val_loss: 0.6943 - val_mean_squared_error: 0.2191\n",
      "Epoch 99/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2971 - mean_squared_error: 0.2170 - val_loss: 0.7072 - val_mean_squared_error: 0.2214\n",
      "Epoch 100/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2759 - mean_squared_error: 0.2137 - val_loss: 0.7276 - val_mean_squared_error: 0.2238\n",
      "Epoch 101/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2751 - mean_squared_error: 0.2157 - val_loss: 0.6297 - val_mean_squared_error: 0.2114\n",
      "Epoch 102/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.3647 - mean_squared_error: 0.2400 - val_loss: 0.6060 - val_mean_squared_error: 0.2046\n",
      "Epoch 103/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.3223 - mean_squared_error: 0.2304 - val_loss: 0.6429 - val_mean_squared_error: 0.2113\n",
      "Epoch 104/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.3229 - mean_squared_error: 0.2280 - val_loss: 0.6753 - val_mean_squared_error: 0.2187\n",
      "Epoch 105/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.3244 - mean_squared_error: 0.2243 - val_loss: 0.7461 - val_mean_squared_error: 0.2309\n",
      "Epoch 106/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.3012 - mean_squared_error: 0.2217 - val_loss: 0.7339 - val_mean_squared_error: 0.2286\n",
      "Epoch 107/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2671 - mean_squared_error: 0.2172 - val_loss: 0.7258 - val_mean_squared_error: 0.2261\n",
      "Epoch 108/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.3064 - mean_squared_error: 0.2171 - val_loss: 0.7304 - val_mean_squared_error: 0.2260\n",
      "Epoch 109/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2933 - mean_squared_error: 0.2157 - val_loss: 0.7078 - val_mean_squared_error: 0.2221\n",
      "Epoch 110/300\n",
      "68273/68273 [==============================] - 53s 779us/step - loss: 2.3110 - mean_squared_error: 0.2150 - val_loss: 0.7054 - val_mean_squared_error: 0.2207\n",
      "Epoch 111/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2811 - mean_squared_error: 0.2131 - val_loss: 0.6954 - val_mean_squared_error: 0.2181\n",
      "Epoch 112/300\n",
      "68273/68273 [==============================] - 53s 779us/step - loss: 2.2738 - mean_squared_error: 0.2119 - val_loss: 0.7086 - val_mean_squared_error: 0.2202\n",
      "Epoch 113/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2950 - mean_squared_error: 0.2126 - val_loss: 0.6991 - val_mean_squared_error: 0.2180\n",
      "Epoch 114/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2769 - mean_squared_error: 0.2098 - val_loss: 0.6851 - val_mean_squared_error: 0.2142\n",
      "Epoch 115/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2551 - mean_squared_error: 0.2095 - val_loss: 0.7061 - val_mean_squared_error: 0.2183\n",
      "Epoch 116/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2673 - mean_squared_error: 0.2090 - val_loss: 0.7253 - val_mean_squared_error: 0.2209\n",
      "Epoch 117/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2622 - mean_squared_error: 0.2080 - val_loss: 0.6792 - val_mean_squared_error: 0.2138\n",
      "Epoch 118/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.2581 - mean_squared_error: 0.2074 - val_loss: 0.7141 - val_mean_squared_error: 0.2176\n",
      "Epoch 119/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2872 - mean_squared_error: 0.2074 - val_loss: 0.7264 - val_mean_squared_error: 0.2197\n",
      "Epoch 120/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2811 - mean_squared_error: 0.2086 - val_loss: 0.7739 - val_mean_squared_error: 0.2264\n",
      "Epoch 121/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2772 - mean_squared_error: 0.2103 - val_loss: 0.6953 - val_mean_squared_error: 0.2158\n",
      "Epoch 122/300\n",
      "68273/68273 [==============================] - 53s 780us/step - loss: 2.2895 - mean_squared_error: 0.2076 - val_loss: 0.7119 - val_mean_squared_error: 0.2185\n",
      "Epoch 123/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.2916 - mean_squared_error: 0.2079 - val_loss: 0.7312 - val_mean_squared_error: 0.2206\n",
      "Epoch 124/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.2967 - mean_squared_error: 0.2067 - val_loss: 0.7513 - val_mean_squared_error: 0.2236\n",
      "Epoch 125/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2783 - mean_squared_error: 0.2066 - val_loss: 0.7032 - val_mean_squared_error: 0.2164\n",
      "Epoch 126/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2799 - mean_squared_error: 0.2064 - val_loss: 0.6986 - val_mean_squared_error: 0.2179\n",
      "Epoch 127/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.2334 - mean_squared_error: 0.2034 - val_loss: 0.7084 - val_mean_squared_error: 0.2189\n",
      "Epoch 128/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2930 - mean_squared_error: 0.2107 - val_loss: 0.7349 - val_mean_squared_error: 0.2218\n",
      "Epoch 129/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.2593 - mean_squared_error: 0.2051 - val_loss: 0.7221 - val_mean_squared_error: 0.2192\n",
      "Epoch 130/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2514 - mean_squared_error: 0.2039 - val_loss: 0.7361 - val_mean_squared_error: 0.2210\n",
      "Epoch 131/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2850 - mean_squared_error: 0.2074 - val_loss: 0.7081 - val_mean_squared_error: 0.2169\n",
      "Epoch 132/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2756 - mean_squared_error: 0.2050 - val_loss: 0.7141 - val_mean_squared_error: 0.2178\n",
      "Epoch 133/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2709 - mean_squared_error: 0.2050 - val_loss: 0.7558 - val_mean_squared_error: 0.2226\n",
      "Epoch 134/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2489 - mean_squared_error: 0.2031 - val_loss: 0.7140 - val_mean_squared_error: 0.2176\n",
      "Epoch 135/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2486 - mean_squared_error: 0.2015 - val_loss: 0.7386 - val_mean_squared_error: 0.2195\n",
      "Epoch 136/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2685 - mean_squared_error: 0.2041 - val_loss: 0.8144 - val_mean_squared_error: 0.2323\n",
      "Epoch 137/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2634 - mean_squared_error: 0.2131 - val_loss: 0.6992 - val_mean_squared_error: 0.2155\n",
      "Epoch 138/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2938 - mean_squared_error: 0.2079 - val_loss: 0.7708 - val_mean_squared_error: 0.2252\n",
      "Epoch 139/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2551 - mean_squared_error: 0.2073 - val_loss: 0.6678 - val_mean_squared_error: 0.2135\n",
      "Epoch 140/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2973 - mean_squared_error: 0.2145 - val_loss: 0.7053 - val_mean_squared_error: 0.2193\n",
      "Epoch 141/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2696 - mean_squared_error: 0.2075 - val_loss: 0.7565 - val_mean_squared_error: 0.2253\n",
      "Epoch 142/300\n",
      "68273/68273 [==============================] - 53s 778us/step - loss: 2.2359 - mean_squared_error: 0.2038 - val_loss: 0.8057 - val_mean_squared_error: 0.2295\n",
      "Epoch 143/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2408 - mean_squared_error: 0.2038 - val_loss: 0.7193 - val_mean_squared_error: 0.2174\n",
      "Epoch 144/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68273/68273 [==============================] - 53s 781us/step - loss: 2.3001 - mean_squared_error: 0.2053 - val_loss: 0.7369 - val_mean_squared_error: 0.2200\n",
      "Epoch 145/300\n",
      "68273/68273 [==============================] - 53s 778us/step - loss: 2.2382 - mean_squared_error: 0.2009 - val_loss: 0.7396 - val_mean_squared_error: 0.2202\n",
      "Epoch 146/300\n",
      "68273/68273 [==============================] - 53s 778us/step - loss: 2.2310 - mean_squared_error: 0.2009 - val_loss: 0.7749 - val_mean_squared_error: 0.2231\n",
      "Epoch 147/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2754 - mean_squared_error: 0.2038 - val_loss: 0.7637 - val_mean_squared_error: 0.2233\n",
      "Epoch 148/300\n",
      "68273/68273 [==============================] - 53s 778us/step - loss: 2.2629 - mean_squared_error: 0.2005 - val_loss: 0.7512 - val_mean_squared_error: 0.2212\n",
      "Epoch 149/300\n",
      "68273/68273 [==============================] - 53s 779us/step - loss: 2.2491 - mean_squared_error: 0.1991 - val_loss: 0.7613 - val_mean_squared_error: 0.2212\n",
      "Epoch 150/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2568 - mean_squared_error: 0.2008 - val_loss: 0.7872 - val_mean_squared_error: 0.2250\n",
      "Epoch 151/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2515 - mean_squared_error: 0.2036 - val_loss: 0.6773 - val_mean_squared_error: 0.2118\n",
      "Epoch 152/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2837 - mean_squared_error: 0.2080 - val_loss: 0.7357 - val_mean_squared_error: 0.2189\n",
      "Epoch 153/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2877 - mean_squared_error: 0.2090 - val_loss: 0.7042 - val_mean_squared_error: 0.2167\n",
      "Epoch 154/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2813 - mean_squared_error: 0.2097 - val_loss: 0.7260 - val_mean_squared_error: 0.2180\n",
      "Epoch 155/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.3044 - mean_squared_error: 0.2157 - val_loss: 0.6618 - val_mean_squared_error: 0.2189\n",
      "Epoch 156/300\n",
      "68273/68273 [==============================] - 53s 781us/step - loss: 2.3072 - mean_squared_error: 0.2242 - val_loss: 0.6417 - val_mean_squared_error: 0.2073\n",
      "Epoch 157/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2866 - mean_squared_error: 0.2141 - val_loss: 0.7296 - val_mean_squared_error: 0.2208\n",
      "Epoch 158/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2612 - mean_squared_error: 0.2079 - val_loss: 0.7654 - val_mean_squared_error: 0.2257\n",
      "Epoch 159/300\n",
      "68273/68273 [==============================] - 53s 780us/step - loss: 2.2673 - mean_squared_error: 0.2057 - val_loss: 0.7712 - val_mean_squared_error: 0.2256\n",
      "Epoch 160/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2793 - mean_squared_error: 0.2047 - val_loss: 0.7813 - val_mean_squared_error: 0.2261\n",
      "Epoch 161/300\n",
      "68273/68273 [==============================] - 53s 777us/step - loss: 2.2565 - mean_squared_error: 0.2011 - val_loss: 0.7820 - val_mean_squared_error: 0.2246\n",
      "Epoch 162/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2298 - mean_squared_error: 0.1989 - val_loss: 0.7377 - val_mean_squared_error: 0.2199\n",
      "Epoch 163/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2303 - mean_squared_error: 0.1994 - val_loss: 0.7063 - val_mean_squared_error: 0.2166\n",
      "Epoch 164/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2614 - mean_squared_error: 0.2011 - val_loss: 0.9518 - val_mean_squared_error: 0.2407\n",
      "Epoch 165/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2988 - mean_squared_error: 0.2238 - val_loss: 0.6708 - val_mean_squared_error: 0.2104\n",
      "Epoch 166/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2981 - mean_squared_error: 0.2180 - val_loss: 0.7298 - val_mean_squared_error: 0.2201\n",
      "Epoch 167/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2713 - mean_squared_error: 0.2097 - val_loss: 0.7758 - val_mean_squared_error: 0.2263\n",
      "Epoch 168/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2722 - mean_squared_error: 0.2058 - val_loss: 0.7846 - val_mean_squared_error: 0.2267\n",
      "Epoch 169/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2474 - mean_squared_error: 0.2033 - val_loss: 0.7651 - val_mean_squared_error: 0.2241\n",
      "Epoch 170/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2668 - mean_squared_error: 0.2031 - val_loss: 0.7608 - val_mean_squared_error: 0.2220\n",
      "Epoch 171/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2512 - mean_squared_error: 0.2014 - val_loss: 0.7732 - val_mean_squared_error: 0.2239\n",
      "Epoch 172/300\n",
      "68273/68273 [==============================] - 53s 778us/step - loss: 2.2558 - mean_squared_error: 0.2023 - val_loss: 0.7497 - val_mean_squared_error: 0.2201\n",
      "Epoch 173/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2329 - mean_squared_error: 0.1986 - val_loss: 0.6994 - val_mean_squared_error: 0.2139\n",
      "Epoch 174/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2708 - mean_squared_error: 0.2013 - val_loss: 0.7798 - val_mean_squared_error: 0.2225\n",
      "Epoch 175/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2518 - mean_squared_error: 0.2001 - val_loss: 0.7369 - val_mean_squared_error: 0.2176\n",
      "Epoch 176/300\n",
      "68273/68273 [==============================] - 53s 777us/step - loss: 2.2662 - mean_squared_error: 0.1998 - val_loss: 0.7598 - val_mean_squared_error: 0.2215\n",
      "Epoch 177/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.2473 - mean_squared_error: 0.1970 - val_loss: 0.7621 - val_mean_squared_error: 0.2204\n",
      "Epoch 178/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2282 - mean_squared_error: 0.1970 - val_loss: 0.7926 - val_mean_squared_error: 0.2238\n",
      "Epoch 179/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.2285 - mean_squared_error: 0.1954 - val_loss: 0.7808 - val_mean_squared_error: 0.2220\n",
      "Epoch 180/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.2296 - mean_squared_error: 0.1972 - val_loss: 0.7210 - val_mean_squared_error: 0.2167\n",
      "Epoch 181/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2225 - mean_squared_error: 0.1958 - val_loss: 0.7997 - val_mean_squared_error: 0.2250\n",
      "Epoch 182/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2357 - mean_squared_error: 0.1945 - val_loss: 0.8073 - val_mean_squared_error: 0.2238\n",
      "Epoch 183/300\n",
      "68273/68273 [==============================] - 53s 777us/step - loss: 2.2312 - mean_squared_error: 0.1945 - val_loss: 0.7518 - val_mean_squared_error: 0.2213\n",
      "Epoch 184/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2410 - mean_squared_error: 0.1999 - val_loss: 0.7997 - val_mean_squared_error: 0.2262\n",
      "Epoch 185/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2071 - mean_squared_error: 0.1942 - val_loss: 0.8241 - val_mean_squared_error: 0.2269\n",
      "Epoch 186/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2445 - mean_squared_error: 0.1953 - val_loss: 0.8246 - val_mean_squared_error: 0.2263\n",
      "Epoch 187/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2149 - mean_squared_error: 0.1932 - val_loss: 0.8093 - val_mean_squared_error: 0.2241\n",
      "Epoch 188/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.2390 - mean_squared_error: 0.1943 - val_loss: 0.7628 - val_mean_squared_error: 0.2206\n",
      "Epoch 189/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2368 - mean_squared_error: 0.1979 - val_loss: 0.7920 - val_mean_squared_error: 0.2229\n",
      "Epoch 190/300\n",
      "68273/68273 [==============================] - 53s 781us/step - loss: 2.2398 - mean_squared_error: 0.1943 - val_loss: 0.9733 - val_mean_squared_error: 0.2422\n",
      "Epoch 191/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2566 - mean_squared_error: 0.2052 - val_loss: 0.7626 - val_mean_squared_error: 0.2212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2669 - mean_squared_error: 0.2007 - val_loss: 0.7857 - val_mean_squared_error: 0.2230\n",
      "Epoch 193/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2333 - mean_squared_error: 0.1961 - val_loss: 0.8399 - val_mean_squared_error: 0.2281\n",
      "Epoch 194/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2584 - mean_squared_error: 0.1993 - val_loss: 0.7870 - val_mean_squared_error: 0.2236\n",
      "Epoch 195/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2141 - mean_squared_error: 0.1940 - val_loss: 0.7874 - val_mean_squared_error: 0.2220\n",
      "Epoch 196/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2576 - mean_squared_error: 0.1937 - val_loss: 0.7725 - val_mean_squared_error: 0.2223\n",
      "Epoch 197/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2334 - mean_squared_error: 0.1935 - val_loss: 0.8153 - val_mean_squared_error: 0.2246\n",
      "Epoch 198/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.1947 - mean_squared_error: 0.1918 - val_loss: 0.7808 - val_mean_squared_error: 0.2215\n",
      "Epoch 199/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.2389 - mean_squared_error: 0.1929 - val_loss: 0.8095 - val_mean_squared_error: 0.2246\n",
      "Epoch 200/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.2341 - mean_squared_error: 0.1911 - val_loss: 0.8473 - val_mean_squared_error: 0.2274\n",
      "Epoch 201/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2299 - mean_squared_error: 0.1926 - val_loss: 0.8100 - val_mean_squared_error: 0.2250\n",
      "Epoch 202/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2331 - mean_squared_error: 0.1914 - val_loss: 0.8198 - val_mean_squared_error: 0.2255\n",
      "Epoch 203/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2220 - mean_squared_error: 0.1917 - val_loss: 0.7650 - val_mean_squared_error: 0.2204\n",
      "Epoch 204/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2133 - mean_squared_error: 0.1923 - val_loss: 0.8352 - val_mean_squared_error: 0.2293\n",
      "Epoch 205/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2232 - mean_squared_error: 0.1926 - val_loss: 0.8335 - val_mean_squared_error: 0.2273\n",
      "Epoch 206/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2282 - mean_squared_error: 0.1903 - val_loss: 0.7632 - val_mean_squared_error: 0.2211\n",
      "Epoch 207/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2262 - mean_squared_error: 0.1903 - val_loss: 0.8223 - val_mean_squared_error: 0.2255\n",
      "Epoch 208/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2062 - mean_squared_error: 0.1904 - val_loss: 0.8590 - val_mean_squared_error: 0.2292\n",
      "Epoch 209/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2168 - mean_squared_error: 0.1888 - val_loss: 0.8208 - val_mean_squared_error: 0.2249\n",
      "Epoch 210/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2231 - mean_squared_error: 0.1885 - val_loss: 0.8397 - val_mean_squared_error: 0.2281\n",
      "Epoch 211/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2439 - mean_squared_error: 0.1955 - val_loss: 0.8090 - val_mean_squared_error: 0.2333\n",
      "Epoch 212/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2723 - mean_squared_error: 0.2042 - val_loss: 0.8463 - val_mean_squared_error: 0.2285\n",
      "Epoch 213/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2188 - mean_squared_error: 0.1931 - val_loss: 0.8269 - val_mean_squared_error: 0.2284\n",
      "Epoch 214/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2103 - mean_squared_error: 0.1904 - val_loss: 0.8715 - val_mean_squared_error: 0.2301\n",
      "Epoch 215/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2273 - mean_squared_error: 0.1946 - val_loss: 0.7841 - val_mean_squared_error: 0.2241\n",
      "Epoch 216/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.2045 - mean_squared_error: 0.1916 - val_loss: 0.8064 - val_mean_squared_error: 0.2238\n",
      "Epoch 217/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.2370 - mean_squared_error: 0.1927 - val_loss: 0.7840 - val_mean_squared_error: 0.2224\n",
      "Epoch 218/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2157 - mean_squared_error: 0.1909 - val_loss: 0.8383 - val_mean_squared_error: 0.2266\n",
      "Epoch 219/300\n",
      "68273/68273 [==============================] - 53s 771us/step - loss: 2.2387 - mean_squared_error: 0.1906 - val_loss: 0.8331 - val_mean_squared_error: 0.2268\n",
      "Epoch 220/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2077 - mean_squared_error: 0.1878 - val_loss: 0.8061 - val_mean_squared_error: 0.2241\n",
      "Epoch 221/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2299 - mean_squared_error: 0.1889 - val_loss: 0.8298 - val_mean_squared_error: 0.2259\n",
      "Epoch 222/300\n",
      "68273/68273 [==============================] - 53s 770us/step - loss: 2.2073 - mean_squared_error: 0.1876 - val_loss: 0.9077 - val_mean_squared_error: 0.2314\n",
      "Epoch 223/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2200 - mean_squared_error: 0.1875 - val_loss: 0.8262 - val_mean_squared_error: 0.2267\n",
      "Epoch 224/300\n",
      "68273/68273 [==============================] - 53s 781us/step - loss: 2.2193 - mean_squared_error: 0.1883 - val_loss: 0.8500 - val_mean_squared_error: 0.2272\n",
      "Epoch 225/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.1836 - mean_squared_error: 0.1851 - val_loss: 0.8163 - val_mean_squared_error: 0.2253\n",
      "Epoch 226/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.3445 - mean_squared_error: 0.2209 - val_loss: 0.6620 - val_mean_squared_error: 0.2190\n",
      "Epoch 227/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.3192 - mean_squared_error: 0.2285 - val_loss: 0.6378 - val_mean_squared_error: 0.2076\n",
      "Epoch 228/300\n",
      "68273/68273 [==============================] - 53s 777us/step - loss: 2.3157 - mean_squared_error: 0.2212 - val_loss: 0.6984 - val_mean_squared_error: 0.2166\n",
      "Epoch 229/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2803 - mean_squared_error: 0.2124 - val_loss: 0.7669 - val_mean_squared_error: 0.2244\n",
      "Epoch 230/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2854 - mean_squared_error: 0.2087 - val_loss: 0.7979 - val_mean_squared_error: 0.2283\n",
      "Epoch 231/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2690 - mean_squared_error: 0.2036 - val_loss: 0.8235 - val_mean_squared_error: 0.2299\n",
      "Epoch 232/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2546 - mean_squared_error: 0.2002 - val_loss: 0.8191 - val_mean_squared_error: 0.2288\n",
      "Epoch 233/300\n",
      "68273/68273 [==============================] - 53s 777us/step - loss: 2.2494 - mean_squared_error: 0.1978 - val_loss: 0.8158 - val_mean_squared_error: 0.2285\n",
      "Epoch 234/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2192 - mean_squared_error: 0.1946 - val_loss: 0.8210 - val_mean_squared_error: 0.2279\n",
      "Epoch 235/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2207 - mean_squared_error: 0.1925 - val_loss: 0.7907 - val_mean_squared_error: 0.2238\n",
      "Epoch 236/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2265 - mean_squared_error: 0.1906 - val_loss: 0.8104 - val_mean_squared_error: 0.2256\n",
      "Epoch 237/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2369 - mean_squared_error: 0.1919 - val_loss: 0.8051 - val_mean_squared_error: 0.2251\n",
      "Epoch 238/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2565 - mean_squared_error: 0.1917 - val_loss: 0.8209 - val_mean_squared_error: 0.2259\n",
      "Epoch 239/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2378 - mean_squared_error: 0.1937 - val_loss: 0.7129 - val_mean_squared_error: 0.2171\n",
      "Epoch 240/300\n",
      "68273/68273 [==============================] - 53s 778us/step - loss: 2.2605 - mean_squared_error: 0.2003 - val_loss: 0.8500 - val_mean_squared_error: 0.2386\n",
      "Epoch 241/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.3154 - mean_squared_error: 0.2212 - val_loss: 0.7801 - val_mean_squared_error: 0.2286\n",
      "Epoch 242/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2828 - mean_squared_error: 0.2187 - val_loss: 0.7259 - val_mean_squared_error: 0.2202\n",
      "Epoch 243/300\n",
      "68273/68273 [==============================] - 53s 772us/step - loss: 2.2886 - mean_squared_error: 0.2098 - val_loss: 0.7828 - val_mean_squared_error: 0.2267\n",
      "Epoch 244/300\n",
      "68273/68273 [==============================] - 53s 777us/step - loss: 2.2554 - mean_squared_error: 0.2028 - val_loss: 0.8276 - val_mean_squared_error: 0.2306\n",
      "Epoch 245/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2470 - mean_squared_error: 0.1998 - val_loss: 0.8138 - val_mean_squared_error: 0.2280\n",
      "Epoch 246/300\n",
      "68273/68273 [==============================] - 53s 780us/step - loss: 2.2526 - mean_squared_error: 0.2086 - val_loss: 0.6977 - val_mean_squared_error: 0.2164\n",
      "Epoch 247/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2701 - mean_squared_error: 0.2065 - val_loss: 0.7384 - val_mean_squared_error: 0.2214\n",
      "Epoch 248/300\n",
      "68273/68273 [==============================] - 54s 784us/step - loss: 2.2572 - mean_squared_error: 0.2011 - val_loss: 0.7821 - val_mean_squared_error: 0.2252\n",
      "Epoch 249/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2465 - mean_squared_error: 0.1973 - val_loss: 0.8014 - val_mean_squared_error: 0.2258\n",
      "Epoch 250/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2581 - mean_squared_error: 0.1959 - val_loss: 0.7785 - val_mean_squared_error: 0.2232\n",
      "Epoch 251/300\n",
      "68273/68273 [==============================] - 53s 780us/step - loss: 2.2327 - mean_squared_error: 0.1951 - val_loss: 0.7667 - val_mean_squared_error: 0.2220\n",
      "Epoch 252/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2372 - mean_squared_error: 0.1946 - val_loss: 0.7678 - val_mean_squared_error: 0.2218\n",
      "Epoch 253/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2275 - mean_squared_error: 0.1913 - val_loss: 0.7840 - val_mean_squared_error: 0.2233\n",
      "Epoch 254/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2487 - mean_squared_error: 0.1927 - val_loss: 0.7659 - val_mean_squared_error: 0.2218\n",
      "Epoch 255/300\n",
      "68273/68273 [==============================] - 53s 777us/step - loss: 2.2555 - mean_squared_error: 0.1981 - val_loss: 0.7725 - val_mean_squared_error: 0.2224\n",
      "Epoch 256/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2140 - mean_squared_error: 0.1925 - val_loss: 0.7988 - val_mean_squared_error: 0.2252\n",
      "Epoch 257/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2307 - mean_squared_error: 0.1913 - val_loss: 0.8036 - val_mean_squared_error: 0.2245\n",
      "Epoch 258/300\n",
      "68273/68273 [==============================] - 53s 783us/step - loss: 2.2483 - mean_squared_error: 0.1907 - val_loss: 0.8234 - val_mean_squared_error: 0.2266\n",
      "Epoch 259/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2303 - mean_squared_error: 0.1892 - val_loss: 0.8222 - val_mean_squared_error: 0.2259\n",
      "Epoch 260/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2426 - mean_squared_error: 0.1885 - val_loss: 0.7849 - val_mean_squared_error: 0.2223\n",
      "Epoch 261/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2168 - mean_squared_error: 0.1885 - val_loss: 0.8411 - val_mean_squared_error: 0.2274\n",
      "Epoch 262/300\n",
      "68273/68273 [==============================] - 53s 782us/step - loss: 2.2865 - mean_squared_error: 0.1956 - val_loss: 0.8270 - val_mean_squared_error: 0.2272\n",
      "Epoch 263/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2309 - mean_squared_error: 0.1896 - val_loss: 0.8088 - val_mean_squared_error: 0.2248\n",
      "Epoch 264/300\n",
      "68273/68273 [==============================] - 53s 779us/step - loss: 2.2081 - mean_squared_error: 0.1857 - val_loss: 0.8142 - val_mean_squared_error: 0.2247\n",
      "Epoch 265/300\n",
      "68273/68273 [==============================] - 53s 777us/step - loss: 2.2229 - mean_squared_error: 0.1878 - val_loss: 0.8438 - val_mean_squared_error: 0.2263\n",
      "Epoch 266/300\n",
      "68273/68273 [==============================] - 53s 778us/step - loss: 2.2146 - mean_squared_error: 0.1861 - val_loss: 0.8007 - val_mean_squared_error: 0.2228\n",
      "Epoch 267/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2066 - mean_squared_error: 0.1875 - val_loss: 0.8079 - val_mean_squared_error: 0.2244\n",
      "Epoch 268/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2140 - mean_squared_error: 0.1878 - val_loss: 0.8298 - val_mean_squared_error: 0.2261\n",
      "Epoch 269/300\n",
      "68273/68273 [==============================] - 53s 778us/step - loss: 2.2079 - mean_squared_error: 0.1853 - val_loss: 0.8581 - val_mean_squared_error: 0.2281\n",
      "Epoch 270/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2242 - mean_squared_error: 0.1850 - val_loss: 0.8573 - val_mean_squared_error: 0.2280\n",
      "Epoch 271/300\n",
      "68273/68273 [==============================] - 53s 777us/step - loss: 2.2078 - mean_squared_error: 0.1846 - val_loss: 0.8499 - val_mean_squared_error: 0.2274\n",
      "Epoch 272/300\n",
      "68273/68273 [==============================] - 53s 778us/step - loss: 2.1986 - mean_squared_error: 0.1838 - val_loss: 0.8464 - val_mean_squared_error: 0.2278\n",
      "Epoch 273/300\n",
      "68273/68273 [==============================] - 53s 777us/step - loss: 2.2143 - mean_squared_error: 0.1845 - val_loss: 0.8617 - val_mean_squared_error: 0.2293\n",
      "Epoch 274/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2403 - mean_squared_error: 0.1955 - val_loss: 0.7895 - val_mean_squared_error: 0.2240\n",
      "Epoch 275/300\n",
      "68273/68273 [==============================] - 53s 780us/step - loss: 2.2347 - mean_squared_error: 0.1885 - val_loss: 0.8343 - val_mean_squared_error: 0.2275\n",
      "Epoch 276/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2131 - mean_squared_error: 0.1845 - val_loss: 0.8388 - val_mean_squared_error: 0.2282\n",
      "Epoch 277/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2631 - mean_squared_error: 0.1995 - val_loss: 0.7525 - val_mean_squared_error: 0.2296\n",
      "Epoch 278/300\n",
      "68273/68273 [==============================] - 53s 778us/step - loss: 2.2924 - mean_squared_error: 0.2084 - val_loss: 0.7254 - val_mean_squared_error: 0.2165\n",
      "Epoch 279/300\n",
      "68273/68273 [==============================] - 53s 774us/step - loss: 2.2464 - mean_squared_error: 0.1967 - val_loss: 0.8610 - val_mean_squared_error: 0.2294\n",
      "Epoch 280/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2232 - mean_squared_error: 0.1907 - val_loss: 0.8542 - val_mean_squared_error: 0.2303\n",
      "Epoch 281/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2258 - mean_squared_error: 0.1886 - val_loss: 0.8687 - val_mean_squared_error: 0.2311\n",
      "Epoch 282/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2309 - mean_squared_error: 0.1880 - val_loss: 0.8707 - val_mean_squared_error: 0.2306\n",
      "Epoch 283/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2029 - mean_squared_error: 0.1853 - val_loss: 0.7788 - val_mean_squared_error: 0.2233\n",
      "Epoch 284/300\n",
      "68273/68273 [==============================] - 53s 777us/step - loss: 2.2203 - mean_squared_error: 0.1862 - val_loss: 0.8942 - val_mean_squared_error: 0.2326\n",
      "Epoch 285/300\n",
      "68273/68273 [==============================] - 53s 777us/step - loss: 2.2072 - mean_squared_error: 0.1864 - val_loss: 0.8846 - val_mean_squared_error: 0.2307\n",
      "Epoch 286/300\n",
      "68273/68273 [==============================] - 53s 779us/step - loss: 2.2196 - mean_squared_error: 0.1850 - val_loss: 0.8387 - val_mean_squared_error: 0.2281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 287/300\n",
      "68273/68273 [==============================] - 53s 777us/step - loss: 2.1738 - mean_squared_error: 0.1821 - val_loss: 0.8905 - val_mean_squared_error: 0.2319\n",
      "Epoch 288/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2020 - mean_squared_error: 0.1845 - val_loss: 0.8663 - val_mean_squared_error: 0.2302\n",
      "Epoch 289/300\n",
      "68273/68273 [==============================] - 53s 777us/step - loss: 2.1977 - mean_squared_error: 0.1857 - val_loss: 0.8066 - val_mean_squared_error: 0.2251\n",
      "Epoch 290/300\n",
      "68273/68273 [==============================] - 53s 779us/step - loss: 2.3091 - mean_squared_error: 0.2134 - val_loss: 0.7328 - val_mean_squared_error: 0.2274\n",
      "Epoch 291/300\n",
      "68273/68273 [==============================] - 53s 778us/step - loss: 2.3359 - mean_squared_error: 0.2200 - val_loss: 0.6705 - val_mean_squared_error: 0.2131\n",
      "Epoch 292/300\n",
      "68273/68273 [==============================] - 53s 779us/step - loss: 2.3043 - mean_squared_error: 0.2136 - val_loss: 0.7193 - val_mean_squared_error: 0.2194\n",
      "Epoch 293/300\n",
      "68273/68273 [==============================] - 54s 784us/step - loss: 2.2760 - mean_squared_error: 0.2086 - val_loss: 0.7748 - val_mean_squared_error: 0.2265\n",
      "Epoch 294/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2631 - mean_squared_error: 0.2032 - val_loss: 0.8024 - val_mean_squared_error: 0.2288\n",
      "Epoch 295/300\n",
      "68273/68273 [==============================] - 53s 773us/step - loss: 2.2579 - mean_squared_error: 0.1984 - val_loss: 0.8331 - val_mean_squared_error: 0.2309\n",
      "Epoch 296/300\n",
      "68273/68273 [==============================] - 53s 779us/step - loss: 2.2429 - mean_squared_error: 0.1963 - val_loss: 0.8453 - val_mean_squared_error: 0.2313\n",
      "Epoch 297/300\n",
      "68273/68273 [==============================] - 53s 778us/step - loss: 2.2490 - mean_squared_error: 0.1945 - val_loss: 0.8274 - val_mean_squared_error: 0.2287\n",
      "Epoch 298/300\n",
      "68273/68273 [==============================] - 53s 777us/step - loss: 2.2163 - mean_squared_error: 0.1916 - val_loss: 0.7716 - val_mean_squared_error: 0.2224\n",
      "Epoch 299/300\n",
      "68273/68273 [==============================] - 53s 775us/step - loss: 2.2349 - mean_squared_error: 0.1914 - val_loss: 0.8088 - val_mean_squared_error: 0.2255\n",
      "Epoch 300/300\n",
      "68273/68273 [==============================] - 53s 776us/step - loss: 2.2111 - mean_squared_error: 0.1876 - val_loss: 0.7774 - val_mean_squared_error: 0.2233\n"
     ]
    }
   ],
   "source": [
    "best_model_path = \"/home/andrei/yandex_ml_track/best_model_weights.h5\" \n",
    "model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([x1_tr, x2_tr, f_tr], y_tr, validation_data=([x1_val, x2_val, f_val], y_val), \n",
    "                 epochs=300, batch_size=1025, shuffle=True, callbacks=[model_checkpoint,], verbose=1)\n",
    "\n",
    "model.load_weights(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/.local/lib/python3.5/site-packages/ipykernel_launcher.py:24: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/andrei/.local/lib/python3.5/site-packages/ipykernel_launcher.py:25: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/andrei/.local/lib/python3.5/site-packages/ipykernel_launcher.py:36: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/andrei/.local/lib/python3.5/site-packages/ipykernel_launcher.py:37: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/andrei/.local/lib/python3.5/site-packages/ipykernel_launcher.py:38: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/home/andrei/.local/lib/python3.5/site-packages/ipykernel_launcher.py:39: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104834, 1) (104834, 100)\n",
      "Shape:  (104834, 204)\n",
      "104834/104834 [==============================] - 32s 303us/step\n"
     ]
    }
   ],
   "source": [
    "t_test, r_test, f = extract_features(test)\n",
    "test_1 = pad_sequences(tokenizer.texts_to_sequences(t_test), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_2 = pad_sequences(tokenizer.texts_to_sequences(r_test), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "result = model.predict([test_1, test_2, f], batch_size=1025, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             context_id  predicted_labels  reply_id\n",
      "104829  281440204584682          0.464357         1\n",
      "104830  281440204584682          0.416324         2\n",
      "104833  281440204584682          0.361383         5\n",
      "104828  281440204584682          0.333109         0\n",
      "104831  281440204584682          0.325704         3\n",
      "104832  281440204584682          0.170056         4\n",
      "104822  281438845749348          0.436052         0\n",
      "104825  281438845749348          0.422779         3\n",
      "104824  281438845749348          0.417736         2\n",
      "104827  281438845749348          0.412484         5\n",
      "104826  281438845749348          0.410793         4\n",
      "104823  281438845749348          0.196164         1\n",
      "104818  281434882461340          0.463625         2\n",
      "104817  281434882461340          0.389298         1\n",
      "104816  281434882461340          0.359067         0\n",
      "104820  281434882461340          0.352215         4\n",
      "104821  281434882461340          0.333618         5\n",
      "104819  281434882461340          0.265449         3\n",
      "104811  281393881037323          0.530447         1\n",
      "104810  281393881037323          0.523394         0\n",
      "104815  281393881037323          0.460142         5\n",
      "104814  281393881037323          0.381786         4\n",
      "104813  281393881037323          0.376489         3\n",
      "104812  281393881037323          0.366192         2\n",
      "104806  281393716321704          0.503529         2\n",
      "104807  281393716321704          0.498478         3\n",
      "104804  281393716321704          0.473428         0\n",
      "104808  281393716321704          0.382452         4\n",
      "104805  281393716321704          0.373784         1\n",
      "104809  281393716321704          0.329015         5\n",
      "...                 ...               ...       ...\n",
      "26          67103817120          0.547017         2\n",
      "29          67103817120          0.526577         5\n",
      "28          67103817120          0.505924         4\n",
      "24          67103817120          0.483533         0\n",
      "25          67103817120          0.468305         1\n",
      "27          67103817120          0.355688         3\n",
      "23          61199474834          0.519113         5\n",
      "21          61199474834          0.404437         3\n",
      "20          61199474834          0.361287         2\n",
      "18          61199474834          0.346518         0\n",
      "19          61199474834          0.335964         1\n",
      "22          61199474834          0.335876         4\n",
      "15          49670324627          0.447756         3\n",
      "16          49670324627          0.430904         4\n",
      "12          49670324627          0.426397         0\n",
      "17          49670324627          0.416753         5\n",
      "13          49670324627          0.412784         1\n",
      "14          49670324627          0.374195         2\n",
      "11          15805100619          0.399690         5\n",
      "10          15805100619          0.336774         4\n",
      "7           15805100619          0.299352         1\n",
      "6           15805100619          0.296543         0\n",
      "8           15805100619          0.232485         2\n",
      "9           15805100619          0.178125         3\n",
      "4            4909294510          0.425331         4\n",
      "2            4909294510          0.405816         2\n",
      "1            4909294510          0.371056         1\n",
      "5            4909294510          0.354396         5\n",
      "0            4909294510          0.349810         0\n",
      "3            4909294510          0.185071         3\n",
      "\n",
      "[104834 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "test_for_predict = test.copy()\n",
    "test_for_predict['predicted_labels'] = result\n",
    "test_for_predict = test_for_predict.reindex(test_for_predict.sort_values(['context_id', 'predicted_labels'],\n",
    "                                                                         ascending=[False, False]).index)\n",
    "print(test_for_predict[['context_id', 'predicted_labels', 'reply_id']])\n",
    "write_to_submission_file(test_for_predict, out_file='/home/andrei/yandex_ml_track/test_yandex1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}